{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Programming with Energy Data\n",
    "\n",
    "Data from the [National Grid ESO API ](https://www.nationalgrideso.com/data-portal/api-guidance). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and installs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "from deepdiff import DeepDiff\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Terms\n",
    "Note: Units for flow are measured in MW. The following terms are descriptions of the time series. There are additional time series included which represent the carbon intensity forecast for 2023 of various regions. \n",
    "\n",
    "- IFA_FLOW: IFA stands for Interconnexion France-Angleterre, which is a subsea electricity link between France and Great Britain that began operating in 1986. It's a joint venture between National Grid and the French Transmission Operator RTE. The system-to-system flow (SSF) of the IFA is calculated and adjusted for interconnector losses to determine the flow. \n",
    "- TSD: This is the Transmission System generation requirement and is equivalent to the Initial Transmission System Outturn (ITSDO) and Transmission System Demand Forecast on BM Reports. Transmission System Demand is equal to the ND plus the additional generation required to meet station load, pump storage pumping and interconnector exports.\n",
    "- VIKING_FLOW: Flow coming from a record-breaking 475-mile-long land and subsea cable connecting British and Danish energy grids for the first time.\n",
    "- IFA2_FLOW: Commissioned in 2021 IFA2 is a 1,000 MW high voltage direct current (HVDC) electrical interconnector between the British and French transmission systems. It is the second link to France that National Grid has developed with RTE.\n",
    "- EMBEDDED_WIND_GENERATION: This is an estimate of the GB wind generation from wind farms which do not have Transmission System metering installed. These wind farms are embedded in the distribution network and invisible to National Grid. Their effect is to suppress the electricity demand during periods of high wind. The true output of these generators is not known so an estimate is provided based on National Grid’s best model.\n",
    "- ND: This is the Great Britain generation requirement and is equivalent to the Initial National Demand Outturn (INDO) and National Demand Forecast as published on BM Reports. National Demand is the sum of metered generation, but excludes generation required to meet station load, pump storage pumping and interconnector exports. National Demand is calculated as a sum of generation based on National Grid operational generation metering. \n",
    "- MOYLE_FLOW: Flow related to The Moyle Interconnector, a 500 megawatt (MW) HVDC link between Scotland and Northern Ireland, running between Auchencrosh in Ayrshire and Ballycronan More in County Antrim. It went into service in 2001\n",
    "- NEMO_FLOW: Flow from Nemo Link, a 1,000 MegaWatt HVDC submarine power cable between Richborough Energy Park in Kent, the United Kingdom and Zeebrugge, Belgium\n",
    "- ELECLINK_FLOW: Flow from ElecLink, a 1,000 MW high-voltage direct current (HVDC) electrical interconnector between the United Kingdom and France, passing through the Channel Tunnel.\n",
    "- PUMP_STORAGE_PUMPING: The demand due to pumping at hydro pump storage units; the -ve signifies pumping load.\n",
    "- EMBEDDED_WIND_CAPACITY: This is National Grid’s best view of the installed embedded wind capacity in GB. This is based on publically available information compiled from a variety of sources and is not the definitive view. It is consistent with the generation estimate provided above. \n",
    "- SETTLEMENT_DATE: Settlement Date. \n",
    "- ENGLAND_WALES_DEMAND: England and Wales Demand, as ND above but on an England and Wales basis.\n",
    "- EMBEDDED_SOLAR_CAPACITY: As embedded wind capacity above, but for solar generation.\n",
    "- SCOTTISH_TRANSFER: Power transfer across the scottish boundaries due to growth in renewable generation capacity.\n",
    "- NON_BM_STOR: Operating reserve for units that are not included in the ND generator definition. This can be in the form of generation or demand reduction.\n",
    "- SETTLEMENT_PERIOD: Settlement Period. \n",
    "- EAST_WEST_FLOW: Flow from the East - West Interconnector between Ireland and Great Britain.\n",
    "- NSL_FLOW: Flow from the North Sea Link (NSL), a joint venture with Norwegian system operator Statnett. Stretching 720 kilometres under the North Sea, at depths of up to 700 metres, NSL is an interconnector capable of sharing up to 1400 megawatts of electricity\n",
    "- BRITNED_FLOW: Flow from the Britned connector, a two-way 1,000 MW high-voltage direct current connection has a length of 260 km and runs from the Isle of Grain (in Kent) to Maasvlakte (near Rotterdam)\n",
    "- _ID: Line ID. \n",
    "- EMBEDDED_SOLAR_GENERATION: As embedded wind generation above, but for solar generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/_3gy4clj6932_zwdwb7jml_m0000gn/T/ipykernel_10079/563482866.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_carbon_2023.drop([\"_full_text\"], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# API Calls to the Britain national grid API. Calling to retrieve historic electricity demand,\n",
    "# interconnector, wind and solar outturn, and carbon intensity data for 2022 and/or 2023.\n",
    "\n",
    "URL = 'https://api.nationalgrideso.com/api/3/action/datastore_search_sql?sql=SELECT * FROM \"bb44a1b5-75b1-4db2-8491-257f23385006\"'\n",
    "response = requests.get(URL).json()\n",
    "URL2 = 'https://api.nationalgrideso.com/api/3/action/datastore_search_sql?sql=SELECT * FROM \"bf5ab335-9b40-4ea4-b93a-ab4af7bce003\"'\n",
    "response2 = requests.get(URL2).json()\n",
    "URL3 = 'https://api.nationalgrideso.com/api/3/action/datastore_search_sql?sql=SELECT * FROM \"3372646d-419f-4599-97a9-6bb4e7e32862\"'\n",
    "response3 = requests.get(URL3).json()\n",
    "URL4 = 'https://api.nationalgrideso.com/api/3/action/datastore_search_sql?sql=SELECT * FROM \"c16b0e19-c02a-44a8-ba05-4db2c0545a2a\"'\n",
    "response4 = requests.get(URL4).json()\n",
    "\n",
    "\n",
    "# Converting responses from json into pandas dataframe\n",
    "df_demand_2022 = pd.json_normalize(\n",
    "    response[\"result\"][\"records\"],\n",
    "    meta=[\n",
    "        \"IFA_FLOW\",\n",
    "        \"TSD\",\n",
    "        \"VIKING_FLOW\",\n",
    "        \"IFA2_FLOW\",\n",
    "        \"EMBEDDED_WIND_GENERATION\",\n",
    "        \"ND\",\n",
    "        \"MOYLE_FLOW\",\n",
    "        \"NEMO_FLOW\",\n",
    "        \"ELECLINK_FLOW\",\n",
    "        \"PUMP_STORAGE_PUMPING\",\n",
    "        \"EMBEDDED_WIND_CAPACITY\",\n",
    "        \"SETTLEMENT_DATE\",\n",
    "        \"ENGLAND_WALES_DEMAND\",\n",
    "        \"EMBEDDED_SOLAR_CAPACITY\",\n",
    "        \"SCOTTISH_TRANSFER\",\n",
    "        \"NON_BM_STOR\",\n",
    "        \"_FULL_TEXT\",\n",
    "        \"SETTLEMENT_PERIOD\",\n",
    "        \"EAST_WEST_FLOW\",\n",
    "        \"NSL_FLOW\",\n",
    "        \"BRITNED_FLOW\",\n",
    "        \"_ID\",\n",
    "        \"EMBEDDED_SOLAR_GENERATION\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_demand_2023 = pd.json_normalize(\n",
    "    response2[\"result\"][\"records\"],\n",
    "    meta=[\n",
    "        \"IFA_FLOW\",\n",
    "        \"TSD\",\n",
    "        \"VIKING_FLOW\",\n",
    "        \"IFA2_FLOW\",\n",
    "        \"EMBEDDED_WIND_GENERATION\",\n",
    "        \"ND\",\n",
    "        \"MOYLE_FLOW\",\n",
    "        \"NEMO_FLOW\",\n",
    "        \"ELECLINK_FLOW\",\n",
    "        \"PUMP_STORAGE_PUMPING\",\n",
    "        \"EMBEDDED_WIND_CAPACITY\",\n",
    "        \"SETTLEMENT_DATE\",\n",
    "        \"ENGLAND_WALES_DEMAND\",\n",
    "        \"EMBEDDED_SOLAR_CAPACITY\",\n",
    "        \"SCOTTISH_TRANSFER\",\n",
    "        \"NON_BM_STOR\",\n",
    "        \"_FULL_TEXT\",\n",
    "        \"SETTLEMENT_PERIOD\",\n",
    "        \"EAST_WEST_FLOW\",\n",
    "        \"NSL_FLOW\",\n",
    "        \"BRITNED_FLOW\",\n",
    "        \"_ID\",\n",
    "        \"EMBEDDED_SOLAR_GENERATION\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_historic_prices_2022 = pd.json_normalize(\n",
    "    response3[\"result\"][\"records\"],\n",
    "    meta=[\n",
    "        \"Settlement Period\",\n",
    "        \"Half-hourly Charge\",\n",
    "        \"Run Type\",\n",
    "        \"Total Daily BSUoS Charge\",\n",
    "        \"_full_text\",\n",
    "        \"BSUoS Price (£/MWh Hour)\",\n",
    "        \"Settlement Day\",\n",
    "        \"_id\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_carbon = pd.json_normalize(\n",
    "    response4[\"result\"][\"records\"],\n",
    "    meta=[\n",
    "        \"East Midlands\",\n",
    "        \"East England\",\n",
    "        \"West Midlands\",\n",
    "        \"North Scotland\",\n",
    "        \"South Scotland\",\n",
    "        \"_full_text\",\n",
    "        \"South West England\",\n",
    "        \"datetime\",\n",
    "        \"North Wales and Merseyside\",\n",
    "        \"North East England\",\n",
    "        \"South East England\",\n",
    "        \"South Wales\",\n",
    "        \"North West England\",\n",
    "        \"Yorkshire\",\n",
    "        \"London\",\n",
    "        \"_id\",\n",
    "        \"South England\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Conversions to datetime for extracting data for specific years\n",
    "df_historic_prices_2022[\"Settlement Day\"] = pd.to_datetime(\n",
    "    df_historic_prices_2022[\"Settlement Day\"]\n",
    ")\n",
    "df_historic_prices_2022 = df_historic_prices_2022[\n",
    "    df_historic_prices_2022[\"Settlement Day\"].dt.year == 2022\n",
    "]\n",
    "\n",
    "df_carbon['datetime'] = pd.to_datetime(\n",
    "    df_carbon['datetime']\n",
    ")\n",
    "df_carbon_2023 = df_carbon[\n",
    "    df_carbon['datetime'].dt.year == 2023\n",
    "]\n",
    "\n",
    "# Dropping unused columns for future concatenation\n",
    "df_demand_2022.drop([\"_full_text\", \"NON_BM_STOR\"], axis=1, inplace=True)\n",
    "df_demand_2023.drop([\"_full_text\", \"NON_BM_STOR\"], axis=1, inplace=True)\n",
    "df_historic_prices_2022.drop([\"Run Type\", \"_full_text\"], axis=1, inplace=True)\n",
    "df_carbon_2023.drop([\"_full_text\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping id columns and now unused date columns. \n",
    "# Only want the time series that will be clustered - each are of size 17520\n",
    "# so the \"_id\" column is able to be dropped. \n",
    "\n",
    "df_demand_2022_noid = df_demand_2022.drop(\n",
    "    [\"_id\", \"SETTLEMENT_DATE\", \"SETTLEMENT_PERIOD\"], axis=1\n",
    ")\n",
    "df_demand_2023_noid = df_demand_2023.drop(\n",
    "    [\"_id\", \"SETTLEMENT_DATE\", \"SETTLEMENT_PERIOD\"], axis=1\n",
    ")\n",
    "df_demand_2023_noid.columns = [str(col) + \"_2\" for col in df_demand_2023_noid.columns]\n",
    "df_historic_prices_2022_noid = df_historic_prices_2022.drop(\n",
    "    [\"Settlement Period\", \"Settlement Day\", \"_id\"], axis=1\n",
    ")\n",
    "df_carbon_2023_noid = df_carbon_2023.drop(\n",
    "    [\"_id\", \"datetime\"], axis=1\n",
    ")\n",
    "\n",
    "# Concatenating the dataframes. \n",
    "df_full = pd.concat(\n",
    "    [\n",
    "        df_historic_prices_2022_noid.reset_index().drop(\"index\", axis=1, inplace=True),\n",
    "        df_demand_2022_noid,\n",
    "        df_demand_2023_noid,\n",
    "        df_carbon_2023_noid.reset_index().drop(\"index\", axis=1, inplace=True)\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Must perform scaling since clustering algorithms works on similarity/distance\n",
    "df_full = StandardScaler().fit_transform(df_full)\n",
    "df_full_transposed = df_full.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must add in a step to check for nulls and other errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to instantiate multiple clustering models as our initial population for the algorithm. This can include the following: \n",
    "- KMeans\n",
    "    - n_clusters: 3, 4, 5, 6\n",
    "    - max_iter: 100, 200, 300, 400\n",
    "    - tol: 0.00001, 0.0001, 0.001, 0.01\n",
    "- KMedoids\n",
    "    - n_clusters: 3, 4, 5, 6\n",
    "    - metric: euclidean, cosine, haversine, l2 \n",
    "    - method: alternate, pam\n",
    "    - max_iter: 100, 200, 300, 400\n",
    "- DBSCAN\n",
    "    - eps: 0.2, 0.5, 1.0\n",
    "    - min_samples: 3, 5, 10\n",
    "    - metric: euclidean, cosine, haversine, l2 \n",
    "- HDBSCAN\n",
    "    - metric: euclidean, cosine, haversine, l2 \n",
    "    - min_samples: 3, 5, 10\n",
    "    - cluster_selection_epsilon: 0.2, 0.5, 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining which parameters are appropriate to adjust for each clustering model\n",
    "\n",
    "model_list = [\"KMeans\", \"KMedoids\", \"DBSCAN\", \"HDBSCAN\"]\n",
    "\n",
    "list_dict_model_params = [\n",
    "    {\"KMeans\": [\"n_clusters\", \"max_iter\", \"tol\"]},\n",
    "    {\"KMedoids\": [\"n_clusters\", \"metric_1\", \"method\", \"max_iter\"]},\n",
    "    {\"DBSCAN\": [\"eps\", \"min_samples\", \"metric_1\"]},\n",
    "    {\"HDBSCAN\": [\"metric_2\", \"min_samples\", \"eps\"]},\n",
    "]\n",
    "\n",
    "# Defining which parameter values each model can take\n",
    "dict_param_values = {\n",
    "    \"n_clusters\": [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"max_iter\": [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    \"tol\": [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "    \"metric_1\": [\"euclidean\", \"cosine\", \"haversine\", \"l2\", \"cityblock\", \"l1\", \"manhattan\"],\n",
    "    \"metric_2\": [\"l2\", \"canberra\", \"manhattan\", \"euclidean\", \"braycurtis\", \"chebyshev\", \"hamming\"],\n",
    "    \"method\": [\"alternate\", \"pam\"],\n",
    "    \"eps\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1, 2, 3, 4],\n",
    "    \"min_samples\": [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for the Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map the model name to instantiating the model\n",
    "def instantiate_model(model_name):\n",
    "    if model_name == \"KMeans\":\n",
    "        model_out = KMeans()\n",
    "    elif model_name == \"KMedoids\":\n",
    "        model_out = KMedoids()\n",
    "    elif model_name == \"DBSCAN\":\n",
    "        model_out = DBSCAN()\n",
    "    else:\n",
    "        model_out = HDBSCAN()\n",
    "    return model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to initialize the population\n",
    "def init_population(\n",
    "    model_params=list_dict_model_params, param_values=dict_param_values, num_models=15\n",
    "):\n",
    "    population = []\n",
    "    population_models = []\n",
    "\n",
    "    # Random selection of models and associated parameters to initialize\n",
    "    for i in range(num_models):\n",
    "        rand1 = random.randint(0, len(model_params) - 1)\n",
    "        curr_model_name = list(model_params[rand1].keys())[0]\n",
    "        curr_model_params = list(model_params[rand1].values())[0]\n",
    "        params_dict = {}\n",
    "\n",
    "        # For each parameter, randomly select values from the dictionary\n",
    "        for param in curr_model_params:\n",
    "            rand2 = random.randint(0, len(param_values[param]) - 1)\n",
    "            curr_param_value = param_values[param][rand2]\n",
    "            if param == \"metric_1\" or param == \"metric_2\":\n",
    "                param = \"metric\"\n",
    "            elif param == \"eps\" and curr_model_name == \"HDBSCAN\":\n",
    "                param = \"cluster_selection_epsilon\"\n",
    "            else:\n",
    "                pass\n",
    "            params_dict[param] = curr_param_value\n",
    "        population.append({curr_model_name: params_dict})\n",
    "\n",
    "    # Take the population dictionary and iterate through to create model objects/instantiate\n",
    "    for i in range(len(population)):\n",
    "        model_instant = instantiate_model(list(population[i].keys())[0])\n",
    "        model_instant.set_params(**list(population[i].values())[0])\n",
    "        population_models.append(model_instant)\n",
    "\n",
    "    return population_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to iterate models, fit, and evaluate cluster results.\n",
    "def cluster_fitness(population, data=df_full_transposed):\n",
    "    fitness_indices = {}\n",
    "    for model in population:\n",
    "\n",
    "        # Try to fit each model and evaluate wtih silhouette score since\n",
    "        # some model fits may fail due to random nature of instantiating\n",
    "        try:\n",
    "            model.fit(data)\n",
    "            curr_score = silhouette_score(data, model.labels_) \n",
    "            fitness_indices[model] = curr_score\n",
    "        except:\n",
    "            fitness_indices[model] = -1  \n",
    "    \n",
    "    # Sort scoring to get most performant models at the top\n",
    "    fitness_indices_sorted = {\n",
    "        k: v\n",
    "        for k, v in sorted(\n",
    "            fitness_indices.items(), reverse=True, key=lambda item: item[1] \n",
    "        )\n",
    "    }\n",
    "\n",
    "    return fitness_indices_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the selection piece of the algorithm\n",
    "def population_selection(dict_fitness, selection_param=0.7):\n",
    "    next_generation = []\n",
    "    \n",
    "    # selecting only the top percentage of models based on parameter\n",
    "    range_val = int(selection_param * len(dict_fitness))\n",
    "    next_generation = dict(itertools.islice(dict_fitness.items(), range_val))\n",
    "\n",
    "    return next_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossover - If there are models of the same model type (e.g. several HDBSCAN)\n",
    "# then take random parameters from one and swap it with another and add it\n",
    "# to the population as a new model. Iterate through the process three times\n",
    "# to generate three new models.\n",
    "\n",
    "\n",
    "def crossover(dict_selected_pop, model_params, repeat=3):\n",
    "    dict_selected_pop_agg = {}\n",
    "    for model in dict_selected_pop.keys():\n",
    "        model_name = type(model).__name__\n",
    "        if model_name in dict_selected_pop_agg.keys():\n",
    "            dict_selected_pop_agg[model_name].append([model])\n",
    "        else:\n",
    "            dict_selected_pop_agg[model_name] = [[model]]\n",
    "\n",
    "    count = 0\n",
    "    while count < repeat:\n",
    "        rand_num = random.randint(0, len(model_params) - 1)\n",
    "        rand_model = list(model_params[rand_num].keys())[0]\n",
    "        try:\n",
    "            len_check = len(dict_selected_pop_agg[rand_model])\n",
    "        except:\n",
    "            len_check = 0\n",
    "        if len_check > 1:\n",
    "            rand_nums = random.sample(\n",
    "                range(0, len(dict_selected_pop_agg[rand_model])), 2\n",
    "            )\n",
    "            model1 = dict_selected_pop_agg[rand_model][rand_nums[0]][0]\n",
    "            model2 = dict_selected_pop_agg[rand_model][rand_nums[1]][0]\n",
    "            half_param_length = int(len(model1.get_params()) / 2)\n",
    "            model1_param_extract = [{k: v} for k, v in model1.get_params().items()][\n",
    "                0:half_param_length\n",
    "            ]\n",
    "            model2_param_extract = [{k: v} for k, v in model2.get_params().items()][\n",
    "                half_param_length::\n",
    "            ]\n",
    "            model_params_combine = model1_param_extract + model2_param_extract\n",
    "            dict_model_params_mapping = {\n",
    "                k: v\n",
    "                for single_dict in model_params_combine\n",
    "                for k, v in single_dict.items()\n",
    "            }\n",
    "            new_model = instantiate_model(rand_model).set_params(\n",
    "                **dict_model_params_mapping\n",
    "            )\n",
    "            count += 1\n",
    "            dict_selected_pop[new_model] = 0\n",
    "\n",
    "    return dict_selected_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutation - Select models at random, then select a hyperparameter at random,\n",
    "# and mutate the parameter to another value in the possible selection list. Iterate\n",
    "# through twice and generate two new models.\n",
    "\n",
    "\n",
    "def mutation(dict_selected_pop, model_params, param_values, repeat=2):\n",
    "    count = 0\n",
    "    while count < repeat:\n",
    "        curr_rand_model = random.choice(list(dict_selected_pop.keys()))\n",
    "        curr_rand_model_name = type(curr_rand_model).__name__\n",
    "        curr_rand_model_params = curr_rand_model.get_params()\n",
    "        rand_param_switch = random.choice(list(curr_rand_model_params.keys()))\n",
    "        if rand_param_switch == \"metric\" and (\n",
    "            curr_rand_model_name == \"KMedoids\" or curr_rand_model_name == \"DBSCAN\"\n",
    "        ):\n",
    "            rand_param_switch = \"metric_1\"\n",
    "        elif rand_param_switch == \"metric\" and curr_rand_model_name == \"HDBSCAN\":\n",
    "            rand_param_switch == \"metric_2\"\n",
    "        elif rand_param_switch == \"cluster_selection_epsilon\":\n",
    "            rand_param_switch == \"eps\"\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            rand_new_value = random.choice(param_values[rand_param_switch])\n",
    "            flag_raise = 0\n",
    "        except:\n",
    "            flag_raise = 1\n",
    "\n",
    "        if flag_raise == 1:\n",
    "            pass\n",
    "        else:\n",
    "            if rand_param_switch == \"metric_1\" or rand_param_switch == \"metric_2\":\n",
    "                rand_param_switch = \"metric\"\n",
    "            elif rand_param_switch == \"eps\" and curr_rand_model_name == \"HDBSCAN\":\n",
    "                rand_param_switch = \"cluster_selection_epsilon\"\n",
    "            else:\n",
    "                pass\n",
    "            curr_rand_model_params[rand_param_switch] = rand_new_value\n",
    "            new_model = instantiate_model(curr_rand_model_name).set_params(\n",
    "                **curr_rand_model_params\n",
    "            )\n",
    "            dict_selected_pop[new_model] = 0\n",
    "            count += 1\n",
    "\n",
    "    return dict_selected_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the evolution function - combines all previous functions and applies them iteratively\n",
    "\n",
    "def evolution(model_params, param_values, init_population_num, df, selection_param, crossover_repeat, mutation_repeat, cutoff_score):\n",
    "    init_pop = init_population(model_params, param_values, init_population_num)\n",
    "    first_fitness_eval = cluster_fitness(population = init_pop, data = df)\n",
    "    score = [element for element in first_fitness_eval.values()][0]\n",
    "    model_hold = {}\n",
    "    if score < cutoff_score: \n",
    "        evaluation_i = first_fitness_eval\n",
    "    else:\n",
    "        return [model for model in first_fitness_eval.keys()][0], score\n",
    "    i = 0\n",
    "    while score < cutoff_score: \n",
    "        if i < 70:\n",
    "            select_i = population_selection(evaluation_i, selection_param = selection_param)\n",
    "            crossover_i = crossover(dict_selected_pop = select_i, model_params = model_params, repeat = crossover_repeat)\n",
    "            mutation_i = mutation(dict_selected_pop = crossover_i, model_params = model_params, param_values = param_values, repeat = mutation_repeat)\n",
    "            next_population = [model_name for model_name in mutation_i.keys()]\n",
    "            evaluation_i = cluster_fitness(population = next_population, data = df)\n",
    "            score = [element for element in evaluation_i.values()][0]\n",
    "            print(f'Top model is {[model for model in evaluation_i.keys()][0]} and associated score is {score}')\n",
    "            model_hold[[model for model in evaluation_i.keys()][0]] = score\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return sorted(model_hold.items(), reverse=True, key=lambda item: item[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top model is KMeans(max_iter=250, n_clusters=10, tol=0.01) and associated score is 0.30248790953532156\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.01) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=250, n_clusters=10, tol=0.01) and associated score is 0.3183130822672847\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.01) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.001) and associated score is 0.32393704954577524\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=1e-05) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.01) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.1) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(max_iter=250, n_clusters=10, tol=1e-05) and associated score is 0.3184300308731838\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.01) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.1) and associated score is 0.31360500918819684\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.01) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(max_iter=250, n_clusters=10, tol=0.01) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=200, n_clusters=10, tol=0.1) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=250, n_clusters=10, tol=0.01) and associated score is 0.3184651114595397\n",
      "Top model is KMeans(max_iter=250, n_clusters=10, tol=0.01) and associated score is 0.3184300308731838\n",
      "Top model is KMeans(max_iter=100, n_clusters=10, tol=0.01) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.1) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=100, n_clusters=10, tol=0.01) and associated score is 0.3184300308731838\n",
      "Top model is KMeans(max_iter=200, n_clusters=10, tol=0.01) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.01) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=200, n_clusters=10, tol=0.001) and associated score is 0.3184300308731838\n",
      "Top model is KMeans(max_iter=100, n_clusters=10, tol=0.1) and associated score is 0.3147187412449993\n",
      "Top model is KMeans(max_iter=100, n_clusters=10, tol=0.01) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=100, n_clusters=10, tol=0.01) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(n_clusters=10, tol=0.01) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(n_clusters=10, tol=0.01) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=100, n_clusters=10, tol=0.001) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=350, n_clusters=10, tol=0.01) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.01) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.01) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(n_clusters=10, tol=0.01) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=100, n_clusters=10, tol=0.1) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=100, n_clusters=10) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(max_iter=100, n_clusters=10) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(n_clusters=10, tol=1e-05) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=100, n_clusters=10, tol=1e-05) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(n_clusters=10) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(n_clusters=10, tol=0.1) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(n_clusters=10, tol=1e-05) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(n_clusters=10, tol=0.1) and associated score is 0.3184300308731838\n",
      "Top model is KMeans(n_clusters=10, tol=0.1) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(n_clusters=10) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=400, n_clusters=10) and associated score is 0.3147187412449993\n",
      "Top model is KMeans(n_clusters=10, tol=0.1) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=400, n_clusters=10) and associated score is 0.3133694575792003\n",
      "Top model is KMeans(max_iter=350, n_clusters=10, tol=0.1) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=100, n_clusters=10) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=350, n_clusters=10, tol=0.1) and associated score is 0.32393704954577524\n",
      "Top model is KMeans(max_iter=350, n_clusters=10, tol=0.1) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(max_iter=100, n_clusters=10, tol=0.1) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(n_clusters=10, tol=0.1) and associated score is 0.3174399968507956\n",
      "Top model is KMeans(n_clusters=10, tol=1e-05) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(n_clusters=10, tol=0.1) and associated score is 0.3147187412449993\n",
      "Top model is KMeans(max_iter=350, n_clusters=10, tol=1e-05) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(max_iter=350, n_clusters=10, tol=0.1) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(n_clusters=10, tol=0.1) and associated score is 0.3147187412449993\n",
      "Top model is KMeans(n_clusters=10, tol=1e-05) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(max_iter=250, n_clusters=10, tol=1e-05) and associated score is 0.3235276820505186\n",
      "Top model is KMeans(n_clusters=10, tol=0.1) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(n_clusters=10, tol=0.1) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=250, n_clusters=10, tol=0.1) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=1e-05) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(n_clusters=10, tol=1e-05) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(n_clusters=10) and associated score is 0.3174399968507956\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.1) and associated score is 0.3133694575792003\n",
      "Top model is KMeans(n_clusters=10, tol=0.01) and associated score is 0.3226796662985824\n",
      "Top model is KMeans(max_iter=400, n_clusters=10, tol=0.1) and associated score is 0.32147683039574276\n",
      "Top model is KMeans(n_clusters=10, tol=0.01) and associated score is 0.3147187412449993\n",
      "Top model is KMeans(n_clusters=10, tol=0.01) and associated score is 0.3235276820505186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(KMeans(max_iter=400, n_clusters=10, tol=0.001), 0.32393704954577524),\n",
       " (KMeans(max_iter=350, n_clusters=10, tol=0.1), 0.32393704954577524),\n",
       " (KMeans(max_iter=400, n_clusters=10, tol=0.1), 0.3235276820505186),\n",
       " (KMeans(max_iter=400, n_clusters=10, tol=0.01), 0.3235276820505186),\n",
       " (KMeans(max_iter=100, n_clusters=10), 0.3235276820505186),\n",
       " (KMeans(n_clusters=10, tol=1e-05), 0.3235276820505186),\n",
       " (KMeans(max_iter=350, n_clusters=10, tol=0.1), 0.3235276820505186),\n",
       " (KMeans(max_iter=350, n_clusters=10, tol=1e-05), 0.3235276820505186),\n",
       " (KMeans(max_iter=350, n_clusters=10, tol=0.1), 0.3235276820505186),\n",
       " (KMeans(n_clusters=10, tol=1e-05), 0.3235276820505186)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evolution(model_params=list_dict_model_params\n",
    "          , param_values=dict_param_values\n",
    "          , init_population_num=20\n",
    "          , df = df_full_transposed\n",
    "          , selection_param=0.5\n",
    "          , crossover_repeat=5\n",
    "          , mutation_repeat=5\n",
    "          , cutoff_score=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
